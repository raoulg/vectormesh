{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6568ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from vectormesh import Vectorizer\n",
    "\n",
    "assets = Path(\"../assets\")\n",
    "tag = next(assets.glob(\"aktes_*/\"))\n",
    "trainpath = tag / \"train\"\n",
    "trainpath, tag.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ca773",
   "metadata": {},
   "source": [
    "I load the aktes dataset from the assets folder, and set up a vectorizer with a huggingface model.\n",
    "You can find more information about the model [here](https://huggingface.co/Gerwin/legal-bert-dutch-english)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2786a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_from_disk(trainpath)\n",
    "model_name = \"Gerwin/legal-bert-dutch-english\"\n",
    "vectorizer = Vectorizer(model_name=model_name, col_name=\"legal_dutch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf70d9",
   "metadata": {},
   "source": [
    "We load the data from the disk, and give the vectorizer the name of the model, and specify a colulmn name.\n",
    "This is usefull later on, because we can add multiple types of vectors to a cache.\n",
    "\n",
    "Lets use a sample, because vectorization can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train.select(range(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc605d",
   "metadata": {},
   "source": [
    "The VectorCache will \n",
    "- loop over the full dataset (in this notebook, the sample. For the full dataset, use hardware accelaration.)\n",
    "- add metadata to keep track of which vectors belong to which dataset and vectorizer\n",
    "- store the vectorcache on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc637ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectormesh import VectorCache\n",
    "\n",
    "vectorcache = VectorCache.create(\n",
    "    cache_dir=Path(\"tmp/artefacts\"),\n",
    "    vectorizer=vectorizer,\n",
    "    dataset=sample,\n",
    "    dataset_tag=tag.name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f95c0",
   "metadata": {},
   "source": [
    "This takes a while! That is why i have already created a cache with this model. \n",
    "\n",
    "Check the vectorcache for some usefull metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorcache.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186f296",
   "metadata": {},
   "source": [
    "Every document is split into chunks of max 512 tokens, with an overlap of about 50 tokens (512 // 10).\n",
    "This means that every document is turned into a 2D tensor with shape (chunks, dim) where dim is the embedding dimension (eg 768 for bert-base-uncased)\n",
    "\n",
    "If we run `.__get_item__` on the vectorcache, by doing `vectorcache[0]`, this is being passed on to the underlying dataset, and we get the first document in the dataset.\n",
    "\n",
    "- \"text\" is the original text\n",
    "- \"target\" is the `rechtsfeit` we need to predict\n",
    "- \"labels\" is the rechtsfeit, turned into a labels `0, 1, 2, ...`\n",
    "- \"legal_ductch\" was the `col_name` we specified when creating the vectorizer, and contains the vectors created by that vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorcache[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232cede",
   "metadata": {},
   "source": [
    "We can now **extend** the existing dataset with more vectors! \n",
    "\n",
    "I created a RegexVectorizer, that checks for references of laws in the text.\n",
    "You can run it on the full trainset, it is pretty fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectormesh import RegexVectorizer\n",
    "from vectormesh.data.vectorizers import (\n",
    "    build_legal_reference_pattern,\n",
    "    harmonize_legal_reference,\n",
    ")\n",
    "\n",
    "# Initialize & fit with training_texts\n",
    "regexvectorizer = RegexVectorizer(\n",
    "    pattern_builder=build_legal_reference_pattern,\n",
    "    harmonizer=harmonize_legal_reference,\n",
    "    min_doc_frequency=15,\n",
    "    max_features=200,\n",
    "    device=\"cpu\",\n",
    "    training_texts=train[\"text\"],  # fit it on the full train set\n",
    ")\n",
    "regexvectorizer.get_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42104a1d",
   "metadata": {},
   "source": [
    "It initialized on the full trainset, and found 123 features. This is determined by\n",
    "\n",
    "1. min_doc_frequency: the minimum amount of documents a feature must appear in\n",
    "2. max_features: the maximum amount of features to be extracted\n",
    "\n",
    "To have an idea what it found, we can print some stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexvectorizer.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60597e",
   "metadata": {},
   "source": [
    "But this was just the \"training\" of the regexvectorizer. We didnt actually store this as data in the vectorcache yet. We can do this now.\n",
    "\n",
    "First, lets pick up the existing vectorcache from disk, such that we can extend it with the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d229dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tag = next(\n",
    "    Path(\"tmp/artefacts\").glob(\"*legal_dutch*/\")\n",
    ")  # next picks the first one it finds\n",
    "new_tag.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664c967",
   "metadata": {},
   "source": [
    "This is the foldername of the dataset in `tmp/artefacts` we want to extend.\n",
    "\n",
    "For the new vectors, the cache will use the `.col_name` from `regexvectorizer`, which you can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd859309",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexvectorizer.col_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3616d62",
   "metadata": {},
   "source": [
    "We can now:\n",
    "- use the existing vectorcache dataset with the huggingface vectors\n",
    "- extend it with the regex vectors\n",
    "- regexvectorizer has a feature `col_name`, which is set to `regex_features` by default. You can change it if you want, for example when you modify the regexes.\n",
    "- save the new cache to tmp/artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_cache = VectorCache.create(\n",
    "    cache_dir=Path(\"tmp/artefacts\"),\n",
    "    vectorizer=regexvectorizer,  # use our new regex vectorizer\n",
    "    dataset=vectorcache.dataset,  # use the existing dataset\n",
    "    dataset_tag=new_tag.name,  # this will check for existing metadata.json in the old folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7609c9f",
   "metadata": {},
   "source": [
    "The data is now extended with the regex vectors!\n",
    "Lets check the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_cache.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff590d",
   "metadata": {},
   "source": [
    "lets have a look at an actual observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4718f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_cache[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e04cd",
   "metadata": {},
   "source": [
    "You see:\n",
    "- The original text\n",
    "- The orignal target to predict (`\"rechtsfeit\"`) , eg `tensor([579])`\n",
    "- The text, embedded by the legal-dutch model as a 2D tensor\n",
    "- the text, encoded with the regexvectorizer, as a binary 1D tensor\n",
    "\n",
    "Check the shape of the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_cache[0][\"legal_dutch\"].shape, updated_cache[0][\"regex_features\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d7762",
   "metadata": {},
   "source": [
    "Lets clean up the tmp/artefacts folder, because we only made it for a small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df11d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"tmp/\", ignore_errors=True)\n",
    "shutil.rmtree(\"logs/\", ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f657c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectormesh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
